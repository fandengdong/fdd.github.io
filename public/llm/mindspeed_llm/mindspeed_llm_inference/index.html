<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>使用 Mindspeed-LLM进行推理 | My work notes</title>
<meta name="keywords" content="">
<meta name="description" content="记录mindspeed-llm框架提供的推理方法，这里记录用Qwen2.5-7B模型推理过程。
准备工作
推理模型的权重，同样需要转换为mcore格式，方法同微调一样，参考这里
开启推理测试
准备好模型权重后，可以直接利用mindspeed-llm提供的推理脚本进行推理测试，脚本位于examples/mcore/qwen25/generate_qwen25_7b_ptd.sh
#!/bin/bash
export CUDA_DEVICE_MAX_CONNECTIONS=1

# please fill these path configurations
CHECKPOINT=&#34;/home/fdd/workspace/models/Qwen/Qwen2.5-7B/mcore_tp1_pp1/&#34;
TOKENIZER_PATH=&#34;/home/fdd/workspace/models/Qwen/Qwen2.5-7B-Instruct/&#34;

# Change for multinode config
MASTER_ADDR=localhost
MASTER_PORT=6000
NNODES=1
NODE_RANK=0
NPUS_PER_NODE=1
WORLD_SIZE=$(($NPUS_PER_NODE*$NNODES))

TP=1
PP=1
SEQ_LENGTH=32768

...


torchrun $DISTRIBUTED_ARGS inference.py \
       --use-mcore-models \
       ...
可以看到对于7B模型，采用单卡就可以进行推理了。另外，启动方式依然为torchrun，启动脚本为inference.py。
推理流程
查看inference.py文件：

from megatron.training.initialize import initialize_megatron
from mindspeed_llm.tasks.inference.module import GPTModelInfer, MegatronModuleForCausalLM

def main():
    initialize_megatron(args_defaults={&#39;no_load_rng&#39;: True,
                                       &#39;no_load_optim&#39;: True})
    args = get_args()

    model = MegatronModuleForCausalLM.from_pretrained(
        model_provider=model_provider,
        pretrained_model_name_or_path=args.load
    )

    task_factory(args, model)


if __name__ == &#34;__main__&#34;:
    main()
可以看到初始化megatron环境的时候，调用了initialize_megatron方法（训练也是调用这个接口来初始化），并且禁用了随机数加载和优化器加载功能，因为推理不需要。另外，调用了MegatronModuleForCausalLM的类方法from_pretrained来初始化模型，其中MegatronModuleForCausalLM是一个用于推理的专有class。最后根据args.task参数，调用task_factory方法来执行任务。
我们进一步看下推理模型的结构，即model_provider内容：
def model_provider(pre_process=True, post_process=True):
    ...
    if args.spec is not None:
            transformer_layer_spec = import_module(args.spec)
        else:
            if use_te:
                transformer_layer_spec = get_gpt_layer_with_transformer_engine_spec(args.num_experts, args.moe_grouped_gemm)
            else:
                transformer_layer_spec = get_gpt_layer_local_spec(args.num_experts, args.moe_grouped_gemm)

        model = GPTModelInfer(
            config=config,
            transformer_layer_spec=transformer_layer_spec,
            vocab_size=args.padded_vocab_size,
            max_sequence_length=args.max_position_embeddings,
            pre_process=pre_process,
            post_process=post_process,
            fp16_lm_cross_entropy=args.fp16_lm_cross_entropy,
            parallel_output=True if args.sequence_parallel else False,
            share_embeddings_and_output_weights=not args.untie_embeddings_and_output_weights,
            position_embedding_type=args.position_embedding_type,
            rotary_percent=args.rotary_percent,
            seq_len_interpolation_factor=args.rotary_seq_len_interpolation_factor
        )
    ...
    return model
这里关键看GPTModelInfer的定义：">
<meta name="author" content="fandengdong">
<link rel="canonical" href="http://localhost:1313/llm/mindspeed_llm/mindspeed_llm_inference/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css" integrity="sha256-NDzEgLn/yPBMy&#43;XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/llm/mindspeed_llm/mindspeed_llm_inference/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script><meta property="og:url" content="http://localhost:1313/llm/mindspeed_llm/mindspeed_llm_inference/">
  <meta property="og:site_name" content="My work notes">
  <meta property="og:title" content="使用 Mindspeed-LLM进行推理">
  <meta property="og:description" content="记录mindspeed-llm框架提供的推理方法，这里记录用Qwen2.5-7B模型推理过程。
准备工作 推理模型的权重，同样需要转换为mcore格式，方法同微调一样，参考这里
开启推理测试 准备好模型权重后，可以直接利用mindspeed-llm提供的推理脚本进行推理测试，脚本位于examples/mcore/qwen25/generate_qwen25_7b_ptd.sh
#!/bin/bash export CUDA_DEVICE_MAX_CONNECTIONS=1 # please fill these path configurations CHECKPOINT=&#34;/home/fdd/workspace/models/Qwen/Qwen2.5-7B/mcore_tp1_pp1/&#34; TOKENIZER_PATH=&#34;/home/fdd/workspace/models/Qwen/Qwen2.5-7B-Instruct/&#34; # Change for multinode config MASTER_ADDR=localhost MASTER_PORT=6000 NNODES=1 NODE_RANK=0 NPUS_PER_NODE=1 WORLD_SIZE=$(($NPUS_PER_NODE*$NNODES)) TP=1 PP=1 SEQ_LENGTH=32768 ... torchrun $DISTRIBUTED_ARGS inference.py \ --use-mcore-models \ ... 可以看到对于7B模型，采用单卡就可以进行推理了。另外，启动方式依然为torchrun，启动脚本为inference.py。
推理流程 查看inference.py文件：
from megatron.training.initialize import initialize_megatron from mindspeed_llm.tasks.inference.module import GPTModelInfer, MegatronModuleForCausalLM def main(): initialize_megatron(args_defaults={&#39;no_load_rng&#39;: True, &#39;no_load_optim&#39;: True}) args = get_args() model = MegatronModuleForCausalLM.from_pretrained( model_provider=model_provider, pretrained_model_name_or_path=args.load ) task_factory(args, model) if __name__ == &#34;__main__&#34;: main() 可以看到初始化megatron环境的时候，调用了initialize_megatron方法（训练也是调用这个接口来初始化），并且禁用了随机数加载和优化器加载功能，因为推理不需要。另外，调用了MegatronModuleForCausalLM的类方法from_pretrained来初始化模型，其中MegatronModuleForCausalLM是一个用于推理的专有class。最后根据args.task参数，调用task_factory方法来执行任务。
我们进一步看下推理模型的结构，即model_provider内容：
def model_provider(pre_process=True, post_process=True): ... if args.spec is not None: transformer_layer_spec = import_module(args.spec) else: if use_te: transformer_layer_spec = get_gpt_layer_with_transformer_engine_spec(args.num_experts, args.moe_grouped_gemm) else: transformer_layer_spec = get_gpt_layer_local_spec(args.num_experts, args.moe_grouped_gemm) model = GPTModelInfer( config=config, transformer_layer_spec=transformer_layer_spec, vocab_size=args.padded_vocab_size, max_sequence_length=args.max_position_embeddings, pre_process=pre_process, post_process=post_process, fp16_lm_cross_entropy=args.fp16_lm_cross_entropy, parallel_output=True if args.sequence_parallel else False, share_embeddings_and_output_weights=not args.untie_embeddings_and_output_weights, position_embedding_type=args.position_embedding_type, rotary_percent=args.rotary_percent, seq_len_interpolation_factor=args.rotary_seq_len_interpolation_factor ) ... return model 这里关键看GPTModelInfer的定义：">
  <meta property="og:locale" content="zh-CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="llm">
    <meta property="article:published_time" content="2025-11-14T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-11-14T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="使用 Mindspeed-LLM进行推理">
<meta name="twitter:description" content="记录mindspeed-llm框架提供的推理方法，这里记录用Qwen2.5-7B模型推理过程。
准备工作
推理模型的权重，同样需要转换为mcore格式，方法同微调一样，参考这里
开启推理测试
准备好模型权重后，可以直接利用mindspeed-llm提供的推理脚本进行推理测试，脚本位于examples/mcore/qwen25/generate_qwen25_7b_ptd.sh
#!/bin/bash
export CUDA_DEVICE_MAX_CONNECTIONS=1

# please fill these path configurations
CHECKPOINT=&#34;/home/fdd/workspace/models/Qwen/Qwen2.5-7B/mcore_tp1_pp1/&#34;
TOKENIZER_PATH=&#34;/home/fdd/workspace/models/Qwen/Qwen2.5-7B-Instruct/&#34;

# Change for multinode config
MASTER_ADDR=localhost
MASTER_PORT=6000
NNODES=1
NODE_RANK=0
NPUS_PER_NODE=1
WORLD_SIZE=$(($NPUS_PER_NODE*$NNODES))

TP=1
PP=1
SEQ_LENGTH=32768

...


torchrun $DISTRIBUTED_ARGS inference.py \
       --use-mcore-models \
       ...
可以看到对于7B模型，采用单卡就可以进行推理了。另外，启动方式依然为torchrun，启动脚本为inference.py。
推理流程
查看inference.py文件：

from megatron.training.initialize import initialize_megatron
from mindspeed_llm.tasks.inference.module import GPTModelInfer, MegatronModuleForCausalLM

def main():
    initialize_megatron(args_defaults={&#39;no_load_rng&#39;: True,
                                       &#39;no_load_optim&#39;: True})
    args = get_args()

    model = MegatronModuleForCausalLM.from_pretrained(
        model_provider=model_provider,
        pretrained_model_name_or_path=args.load
    )

    task_factory(args, model)


if __name__ == &#34;__main__&#34;:
    main()
可以看到初始化megatron环境的时候，调用了initialize_megatron方法（训练也是调用这个接口来初始化），并且禁用了随机数加载和优化器加载功能，因为推理不需要。另外，调用了MegatronModuleForCausalLM的类方法from_pretrained来初始化模型，其中MegatronModuleForCausalLM是一个用于推理的专有class。最后根据args.task参数，调用task_factory方法来执行任务。
我们进一步看下推理模型的结构，即model_provider内容：
def model_provider(pre_process=True, post_process=True):
    ...
    if args.spec is not None:
            transformer_layer_spec = import_module(args.spec)
        else:
            if use_te:
                transformer_layer_spec = get_gpt_layer_with_transformer_engine_spec(args.num_experts, args.moe_grouped_gemm)
            else:
                transformer_layer_spec = get_gpt_layer_local_spec(args.num_experts, args.moe_grouped_gemm)

        model = GPTModelInfer(
            config=config,
            transformer_layer_spec=transformer_layer_spec,
            vocab_size=args.padded_vocab_size,
            max_sequence_length=args.max_position_embeddings,
            pre_process=pre_process,
            post_process=post_process,
            fp16_lm_cross_entropy=args.fp16_lm_cross_entropy,
            parallel_output=True if args.sequence_parallel else False,
            share_embeddings_and_output_weights=not args.untie_embeddings_and_output_weights,
            position_embedding_type=args.position_embedding_type,
            rotary_percent=args.rotary_percent,
            seq_len_interpolation_factor=args.rotary_seq_len_interpolation_factor
        )
    ...
    return model
这里关键看GPTModelInfer的定义：">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "大语言模型 (LLM)",
      "item": "http://localhost:1313/llm/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Mindspeed-LLM",
      "item": "http://localhost:1313/llm/mindspeed_llm/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "使用 Mindspeed-LLM进行推理",
      "item": "http://localhost:1313/llm/mindspeed_llm/mindspeed_llm_inference/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "使用 Mindspeed-LLM进行推理",
  "name": "使用 Mindspeed-LLM进行推理",
  "description": "记录mindspeed-llm框架提供的推理方法，这里记录用Qwen2.5-7B模型推理过程。\n准备工作 推理模型的权重，同样需要转换为mcore格式，方法同微调一样，参考这里\n开启推理测试 准备好模型权重后，可以直接利用mindspeed-llm提供的推理脚本进行推理测试，脚本位于examples/mcore/qwen25/generate_qwen25_7b_ptd.sh\n#!/bin/bash export CUDA_DEVICE_MAX_CONNECTIONS=1 # please fill these path configurations CHECKPOINT=\u0026#34;/home/fdd/workspace/models/Qwen/Qwen2.5-7B/mcore_tp1_pp1/\u0026#34; TOKENIZER_PATH=\u0026#34;/home/fdd/workspace/models/Qwen/Qwen2.5-7B-Instruct/\u0026#34; # Change for multinode config MASTER_ADDR=localhost MASTER_PORT=6000 NNODES=1 NODE_RANK=0 NPUS_PER_NODE=1 WORLD_SIZE=$(($NPUS_PER_NODE*$NNODES)) TP=1 PP=1 SEQ_LENGTH=32768 ... torchrun $DISTRIBUTED_ARGS inference.py \\ --use-mcore-models \\ ... 可以看到对于7B模型，采用单卡就可以进行推理了。另外，启动方式依然为torchrun，启动脚本为inference.py。\n推理流程 查看inference.py文件：\nfrom megatron.training.initialize import initialize_megatron from mindspeed_llm.tasks.inference.module import GPTModelInfer, MegatronModuleForCausalLM def main(): initialize_megatron(args_defaults={\u0026#39;no_load_rng\u0026#39;: True, \u0026#39;no_load_optim\u0026#39;: True}) args = get_args() model = MegatronModuleForCausalLM.from_pretrained( model_provider=model_provider, pretrained_model_name_or_path=args.load ) task_factory(args, model) if __name__ == \u0026#34;__main__\u0026#34;: main() 可以看到初始化megatron环境的时候，调用了initialize_megatron方法（训练也是调用这个接口来初始化），并且禁用了随机数加载和优化器加载功能，因为推理不需要。另外，调用了MegatronModuleForCausalLM的类方法from_pretrained来初始化模型，其中MegatronModuleForCausalLM是一个用于推理的专有class。最后根据args.task参数，调用task_factory方法来执行任务。\n我们进一步看下推理模型的结构，即model_provider内容：\ndef model_provider(pre_process=True, post_process=True): ... if args.spec is not None: transformer_layer_spec = import_module(args.spec) else: if use_te: transformer_layer_spec = get_gpt_layer_with_transformer_engine_spec(args.num_experts, args.moe_grouped_gemm) else: transformer_layer_spec = get_gpt_layer_local_spec(args.num_experts, args.moe_grouped_gemm) model = GPTModelInfer( config=config, transformer_layer_spec=transformer_layer_spec, vocab_size=args.padded_vocab_size, max_sequence_length=args.max_position_embeddings, pre_process=pre_process, post_process=post_process, fp16_lm_cross_entropy=args.fp16_lm_cross_entropy, parallel_output=True if args.sequence_parallel else False, share_embeddings_and_output_weights=not args.untie_embeddings_and_output_weights, position_embedding_type=args.position_embedding_type, rotary_percent=args.rotary_percent, seq_len_interpolation_factor=args.rotary_seq_len_interpolation_factor ) ... return model 这里关键看GPTModelInfer的定义：\n",
  "keywords": [
    
  ],
  "articleBody": "记录mindspeed-llm框架提供的推理方法，这里记录用Qwen2.5-7B模型推理过程。\n准备工作 推理模型的权重，同样需要转换为mcore格式，方法同微调一样，参考这里\n开启推理测试 准备好模型权重后，可以直接利用mindspeed-llm提供的推理脚本进行推理测试，脚本位于examples/mcore/qwen25/generate_qwen25_7b_ptd.sh\n#!/bin/bash export CUDA_DEVICE_MAX_CONNECTIONS=1 # please fill these path configurations CHECKPOINT=\"/home/fdd/workspace/models/Qwen/Qwen2.5-7B/mcore_tp1_pp1/\" TOKENIZER_PATH=\"/home/fdd/workspace/models/Qwen/Qwen2.5-7B-Instruct/\" # Change for multinode config MASTER_ADDR=localhost MASTER_PORT=6000 NNODES=1 NODE_RANK=0 NPUS_PER_NODE=1 WORLD_SIZE=$(($NPUS_PER_NODE*$NNODES)) TP=1 PP=1 SEQ_LENGTH=32768 ... torchrun $DISTRIBUTED_ARGS inference.py \\ --use-mcore-models \\ ... 可以看到对于7B模型，采用单卡就可以进行推理了。另外，启动方式依然为torchrun，启动脚本为inference.py。\n推理流程 查看inference.py文件：\nfrom megatron.training.initialize import initialize_megatron from mindspeed_llm.tasks.inference.module import GPTModelInfer, MegatronModuleForCausalLM def main(): initialize_megatron(args_defaults={'no_load_rng': True, 'no_load_optim': True}) args = get_args() model = MegatronModuleForCausalLM.from_pretrained( model_provider=model_provider, pretrained_model_name_or_path=args.load ) task_factory(args, model) if __name__ == \"__main__\": main() 可以看到初始化megatron环境的时候，调用了initialize_megatron方法（训练也是调用这个接口来初始化），并且禁用了随机数加载和优化器加载功能，因为推理不需要。另外，调用了MegatronModuleForCausalLM的类方法from_pretrained来初始化模型，其中MegatronModuleForCausalLM是一个用于推理的专有class。最后根据args.task参数，调用task_factory方法来执行任务。\n我们进一步看下推理模型的结构，即model_provider内容：\ndef model_provider(pre_process=True, post_process=True): ... if args.spec is not None: transformer_layer_spec = import_module(args.spec) else: if use_te: transformer_layer_spec = get_gpt_layer_with_transformer_engine_spec(args.num_experts, args.moe_grouped_gemm) else: transformer_layer_spec = get_gpt_layer_local_spec(args.num_experts, args.moe_grouped_gemm) model = GPTModelInfer( config=config, transformer_layer_spec=transformer_layer_spec, vocab_size=args.padded_vocab_size, max_sequence_length=args.max_position_embeddings, pre_process=pre_process, post_process=post_process, fp16_lm_cross_entropy=args.fp16_lm_cross_entropy, parallel_output=True if args.sequence_parallel else False, share_embeddings_and_output_weights=not args.untie_embeddings_and_output_weights, position_embedding_type=args.position_embedding_type, rotary_percent=args.rotary_percent, seq_len_interpolation_factor=args.rotary_seq_len_interpolation_factor ) ... return model 这里关键看GPTModelInfer的定义：\nclass GPTModelInfer(GPTModel): def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) self.infer_model = MegatronModuleForCausalLM() def generate(self, input_ids=None, **kwargs): return self.infer_model.generate(input_ids=input_ids, **kwargs) 可以发现GPTModelInfer模型继承了GPTModel，增加了一个输出头MegatronModuleForCausalLM，并且定义了一个新的generate方法，这个方法调用了MegatronModuleForCausalLM的generate方法。注意，GPTModel的输出为模型根据输入预测出的下一个token的输出概率分布，其输出维度为[batch_size, seq_len, vocab_size]，然后通过采样的方法来确定最终选取哪个token作为输出。\n进一步，我们看下self.infer_model.generate()方法的实现，也就是MegatronModuleForCausalLM.generate方法：\nclass MegatronModuleForCausalLMABC(torch.nn.Module, abc.ABC): \"\"\" Megatron specific extensions of torch Module with support for text generation. \"\"\" def __init__(self,) -\u003e None: ... def generate(self, input_ids=None, **kwargs): ... # tokenize the prompts context_tokens_tensor, context_length_tensor = self.tokenize_prompts(tokenizer=self.tokenizer, prompts=input_ids, tokens_to_generate=self.max_new_tokens, max_generate_length=self.max_length, add_BOS=False, broadcast=broadcast) # ======================================= # Get the streaming tokens generator # ======================================= if self.num_beams \u003e 1: token_stream = self.beam_search_or_sampling( args.model[0], tokens=context_tokens_tensor, lengths=context_length_tensor, beam_size=self.num_beams, do_sample=self.do_sample, top_k=self.top_k, top_p=self.top_p, temperature=self.temperature, length_penalty=self.length_penalty, num_return_gen=self.num_return_sequences ) else: token_stream = self.greedy_search_or_sampling( args.model[0], tokens=context_tokens_tensor, lengths=context_length_tensor, do_sample=self.do_sample, top_k=self.top_k, top_p=self.top_p, temperature=self.temperature, return_output_log_probs=self.return_output_log_probs ) # ======================================= # Post processions in order to get final # output texts/tokens # ======================================= return self._token_generator(token_stream) 这里首先将输入的prompt进行tokenize，然后调用不同的采样方法生成token流，最后调用_token_generator方法将token流转换为最终的输出结果。注意模型推理的所有核心的token生成逻辑都在self.beam_search_or_sampling和self.greedy_search_or_sampling方法中。我们以greedy search为例：\ndef generate_tokens_probs_and_return_on_first_stage( model, tokens, lengths, return_output_log_probs=False, do_sample=False, top_k=0, top_p=0.0, temperature=1.0, use_eod_token_for_early_termination=True): \"\"\"Main token generation function. Args: model: no interleaving is supported. tokens: prompt tokens extended to be of size [b, max-sequence-length] lengths: original prompt length, size: [b] return_output_log_probs: flag to calculate the log probability of the generated tokens. Note that the log probability is the one from the original logit. top_k, top_p: top-k and top-p sampling parameters. temperature: sampling temperature. use_eod_token_for_early_termination: if True, do early termination if all the sequences have reached this token. Note: Outside of model, other parameters only need to be available on rank 0. Returns: Note that is size is adjusted to a lower value than max-sequence-length if generation is terminated early. tokens: prompt and generated tokens. size: [b, :] lengths: original prompt length, size: [b] output_log_probs: log probability of the tokens. size: [b, s, vocab_size] \"\"\" # 获取全局参数和分词器。Megatron 使用全局变量来存储配置和组件，便于在不同模块间共享。 args = get_args() tokenizer = get_tokenizer() # 获取批次大小、最小提示长度和最大序列长度。这些用于控制生成过程的循环范围 batch_size = tokens.size(0) min_prompt_length = lengths.min().item() max_sequence_length = tokens.size(1) if max_sequence_length \u003e args.max_position_embeddings: raise ValueError(\"Length of prompt + tokens_to_generate longer than allowed\") if max_sequence_length * batch_size \u003e args.max_tokens_to_oom: raise ValueError(\"Too many tokens. \" + str(max_sequence_length*batch_size) + \" is greater than \" + str(args.max_tokens_to_oom)) # 创建推理上下文和前向步骤对象。这是 Megatron 中用于管理推理过程的组件。 # forward step. inference_context = StaticInferenceContext(batch_size, max_sequence_length) forward_step = ForwardStep(model, inference_context) # 确定终止 token ID。eos_id 是 end-of-sequence ID，eod 是 end-of-document。 # Added termination_id to support the case that we want to terminate the # generation once that id is generated. if hasattr(args, 'eos_id'): termination_id = args.eos_id else: termination_id = tokenizer.eod # =================== # Pre-allocate memory # =================== # 预分配内存。只在流水线的最后一个阶段分配，因为只有那里才有完整的 logits。 # Log probability of the sequence (prompt + generated tokens). output_log_probs = None output_log_probs_size = (batch_size, max_sequence_length - 1, args.padded_vocab_size) # Lengths of generated seuquence including including prompts. generated_sequence_lengths = None if mpu.is_pipeline_last_stage(): if return_output_log_probs: output_log_probs = torch.empty(output_log_probs_size, dtype=torch.float32, device=torch.cuda.current_device()) generated_sequence_lengths = torch.ones( batch_size, dtype=torch.int64, device=torch.cuda.current_device()) * max_sequence_length # 跟踪每个序列是否已完成生成。 # Whether we have reached a termination id. is_generation_done = torch.zeros(batch_size, dtype=torch.uint8, device=torch.cuda.current_device()) # ============= # Run infernece # ============= with torch.no_grad(): # 构建注意力掩码和位置 ID。对于特殊任务 'needlebench' 使用不同的处理方式 if getattr(args, \"task\", False) and args.task[0] == 'needlebench': micro_batch_size, seq_length = tokens.size() attention_mask = None position_ids = torch.arange(seq_length, dtype=torch.long, device=tokens.device) position_ids = position_ids.unsqueeze(0).expand_as(tokens) else: attention_mask, position_ids = _build_attention_mask_and_position_ids( tokens) # 针对特定模型（如 Hunyuan）使用特殊的 pad ID 处理。 if get_args().spec is not None and get_args().spec[0] == \"mindspeed_llm.tasks.models.spec.hunyuan_spec\": pad_id = 127961 attention_mask = tokens.ne(pad_id) # 主生成循环，从最小提示长度开始，逐个生成 token。 prev_context_length = 0 for context_length in range(min_prompt_length, max_sequence_length): # start of megatron_adaptation, here we change sample stratrgy # Pick the slice that we need to pass through the network. # KV Cache 优化：只处理新生成的 token，而不是整个序列（非KV cache方法），提高效率。 if args.use_kv_cache: tokens2use = tokens[:, prev_context_length:context_length] positions2use = position_ids[:, prev_context_length:context_length] if attention_mask is not None: attention_mask2use = attention_mask[ ..., prev_context_length:context_length, :context_length] else: attention_mask2use = None else: tokens2use = tokens positions2use = position_ids attention_mask2use = attention_mask # 执行模型前向计算，得到 logits。 # logits will be meanigful only in the last pipeline stage. logits = forward_step(tokens2use, positions2use, attention_mask2use) # 只在流水线最后一个阶段处理 logits，因为只有那里才有完整的输出。 if mpu.is_pipeline_last_stage(): # Always the last stage should have an output. if logits is None: raise ValueError(\"logits must not be None for pipeline last stage\") # Sample. # 如果采用了kv_cache，logits输出维度为[batch_size, 1, vocab_size]，否则logits的维度为[batch_size, seq_len, vocab_size] # 可以看出KV cache的计算复杂度为O(N),而常规的计算复杂度为O(N^2) if args.use_kv_cache: last_token_logits = logits[:, -1, :] else: last_token_logits = logits[:, context_length - 1, :] # 根据采样策略选择下一个 token。 _, new_sample = _sample_strategy(last_token_logits, do_sample=do_sample, top_k=top_k, top_p=top_p, temperature=temperature) # end of megatron_adaptation # If a prompt length is smaller or equal th current context # length, it means we have started generating tokens started = lengths \u003c= context_length # Update the tokens. tokens[started, context_length] = new_sample[started] # Calculate the log probabilities. if return_output_log_probs: last_token_logits = F.log_softmax(last_token_logits, dim=1) output_log_probs[:, context_length - 1, :] = last_token_logits # Update the tokens on the first stage so the next input to # the network is correct. copy_from_last_to_first_pipeline_stage(batch_size, torch.int64, tokens[:, context_length]) # Update the context length for the next token generation. prev_context_length = context_length # Check if all the sequences have hit the termination_id. done = torch.zeros(1, dtype=torch.uint8, device=torch.cuda.current_device()) if mpu.is_pipeline_last_stage(): # TODO(rprenger) These stopping methods are tokenizer dependent # instead tokenization should be in the inference loop so stop sequences can be used done_token = (new_sample == termination_id).byte() \u0026 \\ started.byte() just_finished = (done_token \u0026 ~is_generation_done).bool() generated_sequence_lengths[just_finished.view(-1)] = \\ context_length + 1 is_generation_done = is_generation_done | done_token done = torch.all(is_generation_done) if get_expert_model_parallel_world_size() \u003e 1: pipeline_world_size = mpu.get_pipeline_model_parallel_world_size() world_size = torch.distributed.get_world_size() last_stage_first_rank = int((pipeline_world_size - 1) * world_size / pipeline_world_size) torch.distributed.broadcast(done, last_stage_first_rank, mpu.get_tensor_and_data_parallel_group()) if output_log_probs is None and not (getattr(args, \"task\", False) and args.task[0] == 'needlebench'): output_log_probs = torch.empty(output_log_probs_size, dtype=torch.float32, device=torch.cuda.current_device()) # 关键部分：yield 生成中间结果，使函数成为 generator。这允许调用者逐步获取生成结果。 yield tokens[:, :(context_length + 1)], lengths, output_log_probs done = broadcast_from_last_pipeline_stage(1, torch.uint8, tensor=done) if use_eod_token_for_early_termination and done: break # =================================================== # Update the length of based on max generated length. # =================================================== tokens = tokens[:, :(context_length + 1)] if mpu.is_pipeline_last_stage(): if return_output_log_probs: output_log_probs = output_log_probs[:, :context_length] # ====================================== # Broadcast to the first pipeline stage. # ====================================== # 将最后一个流水线生成结果广播到第一个流水线阶段，作为新的输入 generated_sequence_lengths = broadcast_from_last_to_first_pipeline_stage( batch_size, torch.int64, generated_sequence_lengths) if return_output_log_probs: output_log_probs_size = (batch_size, context_length, args.padded_vocab_size) output_log_probs = broadcast_from_last_to_first_pipeline_stage( output_log_probs_size, torch.float32, output_log_probs) return tokens, lengths, output_log_probs ",
  "wordCount" : "1070",
  "inLanguage": "en",
  "datePublished": "2025-11-14T00:00:00Z",
  "dateModified": "2025-11-14T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "fandengdong"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/llm/mindspeed_llm/mindspeed_llm_inference/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "My work notes",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="My work notes (Alt + H)">My work notes</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/llm/" title="大语言模型 (LLM)">
                    <span>大语言模型 (LLM)</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/toolbox/" title="工具箱 (toolbox)">
                    <span>工具箱 (toolbox)</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/rl/" title="强化学习 (RL)">
                    <span>强化学习 (RL)</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/" title="欢迎来到我的工作空间">
                    <span>欢迎来到我的工作空间</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      使用 Mindspeed-LLM进行推理
    </h1>
    <div class="post-meta"><span title='2025-11-14 00:00:00 +0000 UTC'>November 14, 2025</span>&nbsp;·&nbsp;<span>fandengdong</span>

</div>
  </header> 
  <div class="post-content"><p>记录mindspeed-llm框架提供的推理方法，这里记录用Qwen2.5-7B模型推理过程。</p>
<h2 id="准备工作">准备工作<a hidden class="anchor" aria-hidden="true" href="#准备工作">#</a></h2>
<p>推理模型的权重，同样需要转换为mcore格式，方法同微调一样，参考<a href="https://fandengdong.github.io/llm/mindspeed_llm/mindspeed_llm_finetune/#%E5%87%86%E5%A4%87%E6%9D%83%E9%87%8D">这里</a></p>
<h2 id="开启推理测试">开启推理测试<a hidden class="anchor" aria-hidden="true" href="#开启推理测试">#</a></h2>
<p>准备好模型权重后，可以直接利用mindspeed-llm提供的推理脚本进行推理测试，脚本位于<code>examples/mcore/qwen25/generate_qwen25_7b_ptd.sh</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="cp">#!/bin/bash
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="nb">export</span> <span class="nv">CUDA_DEVICE_MAX_CONNECTIONS</span><span class="o">=</span><span class="m">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># please fill these path configurations</span>
</span></span><span class="line"><span class="cl"><span class="nv">CHECKPOINT</span><span class="o">=</span><span class="s2">&#34;/home/fdd/workspace/models/Qwen/Qwen2.5-7B/mcore_tp1_pp1/&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nv">TOKENIZER_PATH</span><span class="o">=</span><span class="s2">&#34;/home/fdd/workspace/models/Qwen/Qwen2.5-7B-Instruct/&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Change for multinode config</span>
</span></span><span class="line"><span class="cl"><span class="nv">MASTER_ADDR</span><span class="o">=</span>localhost
</span></span><span class="line"><span class="cl"><span class="nv">MASTER_PORT</span><span class="o">=</span><span class="m">6000</span>
</span></span><span class="line"><span class="cl"><span class="nv">NNODES</span><span class="o">=</span><span class="m">1</span>
</span></span><span class="line"><span class="cl"><span class="nv">NODE_RANK</span><span class="o">=</span><span class="m">0</span>
</span></span><span class="line"><span class="cl"><span class="nv">NPUS_PER_NODE</span><span class="o">=</span><span class="m">1</span>
</span></span><span class="line"><span class="cl"><span class="nv">WORLD_SIZE</span><span class="o">=</span><span class="k">$((</span><span class="nv">$NPUS_PER_NODE</span><span class="o">*</span><span class="nv">$NNODES</span><span class="k">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nv">TP</span><span class="o">=</span><span class="m">1</span>
</span></span><span class="line"><span class="cl"><span class="nv">PP</span><span class="o">=</span><span class="m">1</span>
</span></span><span class="line"><span class="cl"><span class="nv">SEQ_LENGTH</span><span class="o">=</span><span class="m">32768</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">torchrun <span class="nv">$DISTRIBUTED_ARGS</span> inference.py <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>       --use-mcore-models <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>       ...
</span></span></code></pre></div><p>可以看到对于7B模型，采用单卡就可以进行推理了。另外，启动方式依然为torchrun，启动脚本为inference.py。</p>
<h2 id="推理流程">推理流程<a hidden class="anchor" aria-hidden="true" href="#推理流程">#</a></h2>
<p>查看inference.py文件：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">megatron.training.initialize</span> <span class="kn">import</span> <span class="n">initialize_megatron</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">mindspeed_llm.tasks.inference.module</span> <span class="kn">import</span> <span class="n">GPTModelInfer</span><span class="p">,</span> <span class="n">MegatronModuleForCausalLM</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">initialize_megatron</span><span class="p">(</span><span class="n">args_defaults</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;no_load_rng&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                       <span class="s1">&#39;no_load_optim&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">    <span class="n">args</span> <span class="o">=</span> <span class="n">get_args</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">model</span> <span class="o">=</span> <span class="n">MegatronModuleForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">model_provider</span><span class="o">=</span><span class="n">model_provider</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">pretrained_model_name_or_path</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">load</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">task_factory</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&#34;__main__&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">main</span><span class="p">()</span>
</span></span></code></pre></div><p>可以看到初始化megatron环境的时候，调用了initialize_megatron方法（训练也是调用这个接口来初始化），并且禁用了随机数加载和优化器加载功能，因为推理不需要。另外，调用了MegatronModuleForCausalLM的类方法from_pretrained来初始化模型，其中MegatronModuleForCausalLM是一个用于推理的专有class。最后根据args.task参数，调用task_factory方法来执行任务。</p>
<p>我们进一步看下推理模型的结构，即model_provider内容：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">model_provider</span><span class="p">(</span><span class="n">pre_process</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">post_process</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="o">...</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">spec</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">transformer_layer_spec</span> <span class="o">=</span> <span class="n">import_module</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">spec</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">use_te</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">transformer_layer_spec</span> <span class="o">=</span> <span class="n">get_gpt_layer_with_transformer_engine_spec</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">num_experts</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">moe_grouped_gemm</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">transformer_layer_spec</span> <span class="o">=</span> <span class="n">get_gpt_layer_local_spec</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">num_experts</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">moe_grouped_gemm</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">model</span> <span class="o">=</span> <span class="n">GPTModelInfer</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">transformer_layer_spec</span><span class="o">=</span><span class="n">transformer_layer_spec</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">vocab_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">padded_vocab_size</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">max_sequence_length</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">max_position_embeddings</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">pre_process</span><span class="o">=</span><span class="n">pre_process</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">post_process</span><span class="o">=</span><span class="n">post_process</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">fp16_lm_cross_entropy</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">fp16_lm_cross_entropy</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">parallel_output</span><span class="o">=</span><span class="kc">True</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">sequence_parallel</span> <span class="k">else</span> <span class="kc">False</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">share_embeddings_and_output_weights</span><span class="o">=</span><span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">untie_embeddings_and_output_weights</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">position_embedding_type</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">position_embedding_type</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">rotary_percent</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">rotary_percent</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">seq_len_interpolation_factor</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">rotary_seq_len_interpolation_factor</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="o">...</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">model</span>
</span></span></code></pre></div><p>这里关键看GPTModelInfer的定义：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">GPTModelInfer</span><span class="p">(</span><span class="n">GPTModel</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">infer_model</span> <span class="o">=</span> <span class="n">MegatronModuleForCausalLM</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">infer_model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></span></code></pre></div><p>可以发现GPTModelInfer模型继承了GPTModel，增加了一个输出头MegatronModuleForCausalLM，并且定义了一个新的generate方法，这个方法调用了MegatronModuleForCausalLM的generate方法。注意，GPTModel的输出为模型根据输入预测出的下一个token的输出概率分布，其输出维度为[batch_size, seq_len, vocab_size]，然后通过采样的方法来确定最终选取哪个token作为输出。</p>
<p>进一步，我们看下self.infer_model.generate()方法的实现，也就是MegatronModuleForCausalLM.generate方法：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">MegatronModuleForCausalLMABC</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">abc</span><span class="o">.</span><span class="n">ABC</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Megatron specific extensions of torch Module with support
</span></span></span><span class="line"><span class="cl"><span class="s2">    for text generation.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="o">...</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="o">...</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># tokenize the prompts</span>
</span></span><span class="line"><span class="cl">        <span class="n">context_tokens_tensor</span><span class="p">,</span> <span class="n">context_length_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenize_prompts</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                                             <span class="n">prompts</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                                             <span class="n">tokens_to_generate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_new_tokens</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                                             <span class="n">max_generate_length</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_length</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                                             <span class="n">add_BOS</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                                             <span class="n">broadcast</span><span class="o">=</span><span class="n">broadcast</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># =======================================</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Get the streaming tokens generator</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># =======================================</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_beams</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">token_stream</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">beam_search_or_sampling</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">args</span><span class="o">.</span><span class="n">model</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                <span class="n">tokens</span><span class="o">=</span><span class="n">context_tokens_tensor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">lengths</span><span class="o">=</span><span class="n">context_length_tensor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">beam_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_beams</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">do_sample</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_sample</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">top_k</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">top_k</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">top_p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">top_p</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">temperature</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">temperature</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">length_penalty</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">length_penalty</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">num_return_gen</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_return_sequences</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">token_stream</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">greedy_search_or_sampling</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">args</span><span class="o">.</span><span class="n">model</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                <span class="n">tokens</span><span class="o">=</span><span class="n">context_tokens_tensor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">lengths</span><span class="o">=</span><span class="n">context_length_tensor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">do_sample</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_sample</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">top_k</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">top_k</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">top_p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">top_p</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">temperature</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">temperature</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">return_output_log_probs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">return_output_log_probs</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># =======================================</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Post processions in order to get final</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># output texts/tokens</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># =======================================</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_token_generator</span><span class="p">(</span><span class="n">token_stream</span><span class="p">)</span>
</span></span></code></pre></div><p>这里首先将输入的prompt进行tokenize，然后调用不同的采样方法生成token流，最后调用<code>_token_generator</code>方法将token流转换为最终的输出结果。注意模型推理的所有核心的token生成逻辑都在self.beam_search_or_sampling和self.greedy_search_or_sampling方法中。我们以greedy search为例：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">generate_tokens_probs_and_return_on_first_stage</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">model</span><span class="p">,</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">return_output_log_probs</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">top_k</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">temperature</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">use_eod_token_for_early_termination</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;Main token generation function.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Args:
</span></span></span><span class="line"><span class="cl"><span class="s2">        model: no interleaving is supported.
</span></span></span><span class="line"><span class="cl"><span class="s2">        tokens: prompt tokens extended to be of size [b, max-sequence-length]
</span></span></span><span class="line"><span class="cl"><span class="s2">        lengths: original prompt length, size: [b]
</span></span></span><span class="line"><span class="cl"><span class="s2">        return_output_log_probs: flag to calculate the log probability of
</span></span></span><span class="line"><span class="cl"><span class="s2">            the generated tokens. Note that the log probability is the one
</span></span></span><span class="line"><span class="cl"><span class="s2">            from the original logit.
</span></span></span><span class="line"><span class="cl"><span class="s2">        top_k, top_p: top-k and top-p sampling parameters.
</span></span></span><span class="line"><span class="cl"><span class="s2">        temperature: sampling temperature.
</span></span></span><span class="line"><span class="cl"><span class="s2">        use_eod_token_for_early_termination: if True, do early termination if
</span></span></span><span class="line"><span class="cl"><span class="s2">            all the sequences have reached this token.
</span></span></span><span class="line"><span class="cl"><span class="s2">    Note: Outside of model, other parameters only need to be available on
</span></span></span><span class="line"><span class="cl"><span class="s2">          rank 0.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns: Note that is size is adjusted to a lower value than
</span></span></span><span class="line"><span class="cl"><span class="s2">             max-sequence-length if generation is terminated early.
</span></span></span><span class="line"><span class="cl"><span class="s2">        tokens: prompt and generated tokens. size: [b, :]
</span></span></span><span class="line"><span class="cl"><span class="s2">        lengths: original prompt length, size: [b]
</span></span></span><span class="line"><span class="cl"><span class="s2">        output_log_probs: log probability of the tokens. size: [b, s, vocab_size]
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 获取全局参数和分词器。Megatron 使用全局变量来存储配置和组件，便于在不同模块间共享。</span>
</span></span><span class="line"><span class="cl">    <span class="n">args</span> <span class="o">=</span> <span class="n">get_args</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">get_tokenizer</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 获取批次大小、最小提示长度和最大序列长度。这些用于控制生成过程的循环范围</span>
</span></span><span class="line"><span class="cl">    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">tokens</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">min_prompt_length</span> <span class="o">=</span> <span class="n">lengths</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">max_sequence_length</span> <span class="o">=</span> <span class="n">tokens</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">max_sequence_length</span> <span class="o">&gt;</span> <span class="n">args</span><span class="o">.</span><span class="n">max_position_embeddings</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&#34;Length of prompt + tokens_to_generate longer than allowed&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">max_sequence_length</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="o">&gt;</span> <span class="n">args</span><span class="o">.</span><span class="n">max_tokens_to_oom</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&#34;Too many tokens.  &#34;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">max_sequence_length</span><span class="o">*</span><span class="n">batch_size</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&#34; is greater than &#34;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">max_tokens_to_oom</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 创建推理上下文和前向步骤对象。这是 Megatron 中用于管理推理过程的组件。</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># forward step.</span>
</span></span><span class="line"><span class="cl">    <span class="n">inference_context</span> <span class="o">=</span> <span class="n">StaticInferenceContext</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_sequence_length</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">forward_step</span> <span class="o">=</span> <span class="n">ForwardStep</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inference_context</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 确定终止 token ID。eos_id 是 end-of-sequence ID，eod 是 end-of-document。</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Added termination_id to support the case that we want to terminate the</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># generation once that id is generated.</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="s1">&#39;eos_id&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">termination_id</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">eos_id</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">termination_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eod</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># ===================</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Pre-allocate memory</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># ===================</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 预分配内存。只在流水线的最后一个阶段分配，因为只有那里才有完整的 logits。</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Log probability of the sequence (prompt + generated tokens).</span>
</span></span><span class="line"><span class="cl">    <span class="n">output_log_probs</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="n">output_log_probs_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_sequence_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">padded_vocab_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Lengths of generated seuquence including including prompts.</span>
</span></span><span class="line"><span class="cl">    <span class="n">generated_sequence_lengths</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">mpu</span><span class="o">.</span><span class="n">is_pipeline_last_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">return_output_log_probs</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">output_log_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">output_log_probs_size</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                           <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                           <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">        <span class="n">generated_sequence_lengths</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">batch_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">())</span> <span class="o">*</span> <span class="n">max_sequence_length</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 跟踪每个序列是否已完成生成。</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Whether we have reached a termination id.</span>
</span></span><span class="line"><span class="cl">    <span class="n">is_generation_done</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># =============</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Run infernece</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># =============</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 构建注意力掩码和位置 ID。对于特殊任务 &#39;needlebench&#39; 使用不同的处理方式</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="s2">&#34;task&#34;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">task</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;needlebench&#39;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">micro_batch_size</span><span class="p">,</span> <span class="n">seq_length</span> <span class="o">=</span> <span class="n">tokens</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="n">attention_mask</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">            <span class="n">position_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                        <span class="n">device</span><span class="o">=</span><span class="n">tokens</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">position_ids</span> <span class="o">=</span> <span class="n">position_ids</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">attention_mask</span><span class="p">,</span> <span class="n">position_ids</span> <span class="o">=</span> <span class="n">_build_attention_mask_and_position_ids</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">tokens</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 针对特定模型（如 Hunyuan）使用特殊的 pad ID 处理。</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">get_args</span><span class="p">()</span><span class="o">.</span><span class="n">spec</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">get_args</span><span class="p">()</span><span class="o">.</span><span class="n">spec</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&#34;mindspeed_llm.tasks.models.spec.hunyuan_spec&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">pad_id</span> <span class="o">=</span> <span class="mi">127961</span>
</span></span><span class="line"><span class="cl">            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">tokens</span><span class="o">.</span><span class="n">ne</span><span class="p">(</span><span class="n">pad_id</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 主生成循环，从最小提示长度开始，逐个生成 token。</span>
</span></span><span class="line"><span class="cl">        <span class="n">prev_context_length</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">context_length</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">min_prompt_length</span><span class="p">,</span> <span class="n">max_sequence_length</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># start of megatron_adaptation, here we change sample stratrgy</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Pick the slice that we need to pass through the network.</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># KV Cache 优化：只处理新生成的 token，而不是整个序列（非KV cache方法），提高效率。</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">use_kv_cache</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">tokens2use</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">[:,</span> <span class="n">prev_context_length</span><span class="p">:</span><span class="n">context_length</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">                <span class="n">positions2use</span> <span class="o">=</span> <span class="n">position_ids</span><span class="p">[:,</span> <span class="n">prev_context_length</span><span class="p">:</span><span class="n">context_length</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">attention_mask2use</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="p">[</span>
</span></span><span class="line"><span class="cl">                        <span class="o">...</span><span class="p">,</span> <span class="n">prev_context_length</span><span class="p">:</span><span class="n">context_length</span><span class="p">,</span> <span class="p">:</span><span class="n">context_length</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">                <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">attention_mask2use</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">tokens2use</span> <span class="o">=</span> <span class="n">tokens</span>
</span></span><span class="line"><span class="cl">                <span class="n">positions2use</span> <span class="o">=</span> <span class="n">position_ids</span>
</span></span><span class="line"><span class="cl">                <span class="n">attention_mask2use</span> <span class="o">=</span> <span class="n">attention_mask</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># 执行模型前向计算，得到 logits。</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># logits will be meanigful only in the last pipeline stage.</span>
</span></span><span class="line"><span class="cl">            <span class="n">logits</span> <span class="o">=</span> <span class="n">forward_step</span><span class="p">(</span><span class="n">tokens2use</span><span class="p">,</span> <span class="n">positions2use</span><span class="p">,</span> <span class="n">attention_mask2use</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># 只在流水线最后一个阶段处理 logits，因为只有那里才有完整的输出。</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">mpu</span><span class="o">.</span><span class="n">is_pipeline_last_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># Always the last stage should have an output.</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">logits</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&#34;logits must not be None for pipeline last stage&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                <span class="c1"># Sample.</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># 如果采用了kv_cache，logits输出维度为[batch_size, 1, vocab_size]，否则logits的维度为[batch_size, seq_len, vocab_size]</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># 可以看出KV cache的计算复杂度为O(N),而常规的计算复杂度为O(N^2)</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">use_kv_cache</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">last_token_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
</span></span><span class="line"><span class="cl">                <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">last_token_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[:,</span> <span class="n">context_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                <span class="c1"># 根据采样策略选择下一个 token。</span>
</span></span><span class="line"><span class="cl">                <span class="n">_</span><span class="p">,</span> <span class="n">new_sample</span> <span class="o">=</span> <span class="n">_sample_strategy</span><span class="p">(</span><span class="n">last_token_logits</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                    <span class="n">do_sample</span><span class="o">=</span><span class="n">do_sample</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                    <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                    <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                    <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                <span class="c1"># end of megatron_adaptation</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                <span class="c1"># If a prompt length is smaller or equal th current context</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># length, it means we have started generating tokens</span>
</span></span><span class="line"><span class="cl">                <span class="n">started</span> <span class="o">=</span> <span class="n">lengths</span> <span class="o">&lt;=</span> <span class="n">context_length</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># Update the tokens.</span>
</span></span><span class="line"><span class="cl">                <span class="n">tokens</span><span class="p">[</span><span class="n">started</span><span class="p">,</span> <span class="n">context_length</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_sample</span><span class="p">[</span><span class="n">started</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                <span class="c1"># Calculate the log probabilities.</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">return_output_log_probs</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">last_token_logits</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">last_token_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">output_log_probs</span><span class="p">[:,</span> <span class="n">context_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">last_token_logits</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># Update the tokens on the first stage so the next input to</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># the network is correct.</span>
</span></span><span class="line"><span class="cl">            <span class="n">copy_from_last_to_first_pipeline_stage</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                   <span class="n">tokens</span><span class="p">[:,</span> <span class="n">context_length</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># Update the context length for the next token generation.</span>
</span></span><span class="line"><span class="cl">            <span class="n">prev_context_length</span> <span class="o">=</span> <span class="n">context_length</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># Check if all the sequences have hit the termination_id.</span>
</span></span><span class="line"><span class="cl">            <span class="n">done</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">mpu</span><span class="o">.</span><span class="n">is_pipeline_last_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># TODO(rprenger) These stopping methods are tokenizer dependent</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># instead tokenization should be in the inference loop so stop sequences can be used</span>
</span></span><span class="line"><span class="cl">                <span class="n">done_token</span> <span class="o">=</span> <span class="p">(</span><span class="n">new_sample</span> <span class="o">==</span> <span class="n">termination_id</span><span class="p">)</span><span class="o">.</span><span class="n">byte</span><span class="p">()</span> <span class="o">&amp;</span> \
</span></span><span class="line"><span class="cl">                        <span class="n">started</span><span class="o">.</span><span class="n">byte</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                <span class="n">just_finished</span> <span class="o">=</span> <span class="p">(</span><span class="n">done_token</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">is_generation_done</span><span class="p">)</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                <span class="n">generated_sequence_lengths</span><span class="p">[</span><span class="n">just_finished</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> \
</span></span><span class="line"><span class="cl">                    <span class="n">context_length</span> <span class="o">+</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">                <span class="n">is_generation_done</span> <span class="o">=</span> <span class="n">is_generation_done</span> <span class="o">|</span> <span class="n">done_token</span>
</span></span><span class="line"><span class="cl">                <span class="n">done</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">is_generation_done</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">get_expert_model_parallel_world_size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">pipeline_world_size</span> <span class="o">=</span> <span class="n">mpu</span><span class="o">.</span><span class="n">get_pipeline_model_parallel_world_size</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                    <span class="n">world_size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                    <span class="n">last_stage_first_rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="n">pipeline_world_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">world_size</span> <span class="o">/</span> <span class="n">pipeline_world_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span><span class="n">done</span><span class="p">,</span> <span class="n">last_stage_first_rank</span><span class="p">,</span> <span class="n">mpu</span><span class="o">.</span><span class="n">get_tensor_and_data_parallel_group</span><span class="p">())</span>                  
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">output_log_probs</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="s2">&#34;task&#34;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">task</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;needlebench&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">output_log_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">output_log_probs_size</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                        <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                        <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 关键部分：yield 生成中间结果，使函数成为 generator。这允许调用者逐步获取生成结果。</span>
</span></span><span class="line"><span class="cl">            <span class="k">yield</span> <span class="n">tokens</span><span class="p">[:,</span> <span class="p">:(</span><span class="n">context_length</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)],</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">output_log_probs</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">done</span> <span class="o">=</span> <span class="n">broadcast_from_last_pipeline_stage</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                      <span class="n">tensor</span><span class="o">=</span><span class="n">done</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">use_eod_token_for_early_termination</span> <span class="ow">and</span> <span class="n">done</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">break</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># ===================================================</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Update the length of based on max generated length.</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># ===================================================</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">[:,</span> <span class="p">:(</span><span class="n">context_length</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">mpu</span><span class="o">.</span><span class="n">is_pipeline_last_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">return_output_log_probs</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">output_log_probs</span> <span class="o">=</span> <span class="n">output_log_probs</span><span class="p">[:,</span> <span class="p">:</span><span class="n">context_length</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># ======================================</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Broadcast to the first pipeline stage.</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># ======================================</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 将最后一个流水线生成结果广播到第一个流水线阶段，作为新的输入</span>
</span></span><span class="line"><span class="cl">    <span class="n">generated_sequence_lengths</span> <span class="o">=</span> <span class="n">broadcast_from_last_to_first_pipeline_stage</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">batch_size</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">generated_sequence_lengths</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">return_output_log_probs</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">output_log_probs_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">context_length</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">padded_vocab_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">output_log_probs</span> <span class="o">=</span> <span class="n">broadcast_from_last_to_first_pipeline_stage</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">output_log_probs_size</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">output_log_probs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">output_log_probs</span>
</span></span></code></pre></div>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">My work notes</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
