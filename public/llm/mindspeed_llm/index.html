<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Mindspeed-LLM | My work notes</title>
<meta name="keywords" content="">
<meta name="description" content="关于Mindspeed-LLM的笔记，这是一个为华为昇腾设备设计的大语言模型的预训练和后训练库">
<meta name="author" content="fandengdong">
<link rel="canonical" href="http://localhost:1313/llm/mindspeed_llm/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css" integrity="sha256-NDzEgLn/yPBMy&#43;XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" type="application/rss+xml" href="http://localhost:1313/llm/mindspeed_llm/index.xml" title="rss">
<link rel="alternate" hreflang="en" href="http://localhost:1313/llm/mindspeed_llm/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script><meta property="og:url" content="http://localhost:1313/llm/mindspeed_llm/">
  <meta property="og:site_name" content="My work notes">
  <meta property="og:title" content="Mindspeed-LLM">
  <meta property="og:description" content="关于Mindspeed-LLM的笔记，这是一个为华为昇腾设备设计的大语言模型的预训练和后训练库">
  <meta property="og:locale" content="zh-CN">
  <meta property="og:type" content="website">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Mindspeed-LLM">
<meta name="twitter:description" content="关于Mindspeed-LLM的笔记，这是一个为华为昇腾设备设计的大语言模型的预训练和后训练库">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "大语言模型 (LLM)",
      "item": "http://localhost:1313/llm/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Mindspeed-LLM",
      "item": "http://localhost:1313/llm/mindspeed_llm/"
    }
  ]
}
</script>
</head>
<body class="list" id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="My work notes (Alt + H)">My work notes</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/llm/" title="大语言模型 (LLM)">
                    <span>大语言模型 (LLM)</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/toolbox/" title="工具箱 (toolbox)">
                    <span>工具箱 (toolbox)</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/rl/" title="强化学习 (RL)">
                    <span>强化学习 (RL)</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/" title="欢迎来到我的工作空间">
                    <span>欢迎来到我的工作空间</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main"> 
<header class="page-header">
  <h1>
    Mindspeed-LLM
  </h1>
  <div class="post-description">
    关于Mindspeed-LLM的笔记，这是一个为华为昇腾设备设计的大语言模型的预训练和后训练库
  </div>
</header>
<div class="post-content"><p>本节包含关于Mindspeed-LLM的笔记和资源，Mindspeed-LLM是一个高性能加速库，专门为华为昇腾设备上的大语言模型设计的pretrain和posttrain库。</p>


</div>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">使用 Mindspeed-LLM进行推理
    </h2>
  </header>
  <div class="entry-content">
    <p>记录mindspeed-llm框架提供的推理方法，这里记录用Qwen2.5-7B模型推理过程。
准备工作 推理模型的权重，同样需要转换为mcore格式，方法同微调一样，参考这里
开启推理测试 准备好模型权重后，可以直接利用mindspeed-llm提供的推理脚本进行推理测试，脚本位于examples/mcore/qwen25/generate_qwen25_7b_ptd.sh
#!/bin/bash export CUDA_DEVICE_MAX_CONNECTIONS=1 # please fill these path configurations CHECKPOINT=&#34;/home/fdd/workspace/models/Qwen/Qwen2.5-7B/mcore_tp1_pp1/&#34; TOKENIZER_PATH=&#34;/home/fdd/workspace/models/Qwen/Qwen2.5-7B-Instruct/&#34; # Change for multinode config MASTER_ADDR=localhost MASTER_PORT=6000 NNODES=1 NODE_RANK=0 NPUS_PER_NODE=1 WORLD_SIZE=$(($NPUS_PER_NODE*$NNODES)) TP=1 PP=1 SEQ_LENGTH=32768 ... torchrun $DISTRIBUTED_ARGS inference.py \ --use-mcore-models \ ... 可以看到对于7B模型，采用单卡就可以进行推理了。另外，启动方式依然为torchrun，启动脚本为inference.py。
推理流程 查看inference.py文件：
from megatron.training.initialize import initialize_megatron from mindspeed_llm.tasks.inference.module import GPTModelInfer, MegatronModuleForCausalLM def main(): initialize_megatron(args_defaults={&#39;no_load_rng&#39;: True, &#39;no_load_optim&#39;: True}) args = get_args() model = MegatronModuleForCausalLM.from_pretrained( model_provider=model_provider, pretrained_model_name_or_path=args.load ) task_factory(args, model) if __name__ == &#34;__main__&#34;: main() 可以看到初始化megatron环境的时候，调用了initialize_megatron方法（训练也是调用这个接口来初始化），并且禁用了随机数加载和优化器加载功能，因为推理不需要。另外，调用了MegatronModuleForCausalLM的类方法from_pretrained来初始化模型，其中MegatronModuleForCausalLM是一个用于推理的专有class。最后根据args.task参数，调用task_factory方法来执行任务。
我们进一步看下推理模型的结构，即model_provider内容：
def model_provider(pre_process=True, post_process=True): ... if args.spec is not None: transformer_layer_spec = import_module(args.spec) else: if use_te: transformer_layer_spec = get_gpt_layer_with_transformer_engine_spec(args.num_experts, args.moe_grouped_gemm) else: transformer_layer_spec = get_gpt_layer_local_spec(args.num_experts, args.moe_grouped_gemm) model = GPTModelInfer( config=config, transformer_layer_spec=transformer_layer_spec, vocab_size=args.padded_vocab_size, max_sequence_length=args.max_position_embeddings, pre_process=pre_process, post_process=post_process, fp16_lm_cross_entropy=args.fp16_lm_cross_entropy, parallel_output=True if args.sequence_parallel else False, share_embeddings_and_output_weights=not args.untie_embeddings_and_output_weights, position_embedding_type=args.position_embedding_type, rotary_percent=args.rotary_percent, seq_len_interpolation_factor=args.rotary_seq_len_interpolation_factor ) ... return model 这里关键看GPTModelInfer的定义：
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-11-14 00:00:00 +0000 UTC'>November 14, 2025</span>&nbsp;·&nbsp;<span>fandengdong</span></footer>
  <a class="entry-link" aria-label="post link to 使用 Mindspeed-LLM进行推理" href="http://localhost:1313/llm/mindspeed_llm/mindspeed_llm_inference/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">使用 Mindspeed-LLM进行预训练
    </h2>
  </header>
  <div class="entry-content">
    <p>本指南记录采用mindspeed-llm进行训练的步骤，以微调为例。
准备数据 准备数据集的jsonl文件，每个样本是一个字典，至少要包含关键字prompt和response，其中prompt为输入，response为输出。
注意：如果提供的jsonl文件里面没有包含chat template，那么在转换数据的时候要添加prompt-type参数来提供模板。下面提供一个带了chat templated的jsonl文件样例：
{&#34;prompt&#34;:&#34;&lt;|im_start|&gt;system\nYou are a helpful assistant. To answer the user\&#39;s question, you first think about the reasoning process and then provide the user with the answer. The reasoning process and answer are enclosed within &lt;think&gt; &lt;\/think&gt; and &lt;answer&gt; &lt;\/answer&gt; tags, respectively, i.e., &lt;think&gt; reasoning process here &lt;\/think&gt; &lt;answer&gt; answer here &lt;\/answer&gt;.&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nLet $\\triangle ABC$ have circumcenter $O$ and incenter $I$ with $\\overline{IA}\\perp\\overline{OI}$, circumradius $13$, and inradius $6$. Find $AB\\cdot AC$.\nPlease reason step by step, and put your final answer within \\boxed{}.\nPlease reason step by step, and put your final answer within \\boxed{}.&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n&#34;,&#34;response&#34;:&#34;&lt;think&gt;\nI have a geometry problem. We have a triangle ABC with circumcenter O and incenter I....&#34;} {&#34;prompt&#34;:&#34;&lt;|im_start|&gt;system\nYou are a helpful assistant. To answer the user\&#39;s question, you first think about the reasoning process and then provide the user with the answer. The reasoning process and answer are enclosed within &lt;think&gt; &lt;\/think&gt; and &lt;answer&gt; &lt;\/answer&gt; tags, respectively, i.e., &lt;think&gt; reasoning process here &lt;\/think&gt; &lt;answer&gt; answer here &lt;\/answer&gt;.&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nLet \\(b\\ge 2\\) be an integer. Call a positive integer \\(n\\) \\(b\\text-\\textit{eautiful}\\) if it has exactly two digits when expressed in base \\(b\\) and these two digits sum to \\(\\sqrt n\\). For example, \\(81\\) is \\(13\\text-\\textit{eautiful}\\) because \\(81 = \\underline{6} \\ \\underline{3}_{13} \\) and \\(6 &#43; 3 = \\sqrt{81}\\). Find the least integer \\(b\\ge 2\\) for which there are more than ten \\(b\\text-\\textit{eautiful}\\) integers.\nPlease reason step by step, and put your final answer within \\boxed{}.\nPlease reason step by step, and put your final answer within \\boxed{}.&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n&#34;,&#34;response&#34;:&#34;&lt;think&gt;\nThe problem defines a \&#34;b-beautiful\&#34; number as a positive integer...&#34;} 准备好jsonl文件后，利用mindspeed-llm自带的脚本preprocess_data.py来预处理数据：
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-11-14 00:00:00 +0000 UTC'>November 14, 2025</span>&nbsp;·&nbsp;<span>fandengdong</span></footer>
  <a class="entry-link" aria-label="post link to 使用 Mindspeed-LLM进行预训练" href="http://localhost:1313/llm/mindspeed_llm/mindspeed_llm_finetune/"></a>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">My work notes</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
